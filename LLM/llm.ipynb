{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1933f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import Document\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Literal, Dict, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR_LANGSMITH_API_KEY\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"YOUR_LANGSMITH_PROJECT\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR_HUGGINGFACEHUB_API_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "qwen = ChatOllama(model=\"qwen2:1.5b\", temperature=0)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ddfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "PERSIST_DIRECTORY = os.path.join(BASE_DIR, \"vector-db\", \"lhdn_db_new\")\n",
    "COLLECTION_NAME = \"lhdn\"\n",
    "\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=PERSIST_DIRECTORY,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b93f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from pydantic import BaseModel, ValidationError, Field\n",
    "import json\n",
    "import time\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"vectorstore\", \"llm\"]\n",
    "\n",
    "\n",
    "# LLM (Ollama)\n",
    "\n",
    "\n",
    "# Improved Prompt (clear, compact, and JSON-structured output)\n",
    "system = \"\"\"\n",
    "You are an intelligent assistant for LHDN (Lembaga Hasil Dalam Negeri Malaysia).\n",
    "\n",
    "Your task is to decide whether a user's question should be answered using a factual vector database (which includes complete information about Malaysian tax for individuals, companies, and e-invoicing), or by the LLM for general conversation.\n",
    "\n",
    "Use the chat history below if helpful:\n",
    "{chat_history}\n",
    "\n",
    "Rules:\n",
    "- If the question is about LHDN, taxes (personal, company), or e-invoicing → respond:\n",
    "  {{ \"datasource\": \"vectorstore\" }}\n",
    "- If the question is small talk, greeting, opinion-based, or unrelated to LHDN/tax/e-invoice → respond:\n",
    "  {{ \"datasource\": \"llm\" }}\n",
    "\n",
    "Respond with ONLY a single line of JSON (no explanation), like:\n",
    "{{ \"datasource\": \"vectorstore\" }}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(system)\n",
    "human_template  = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "route_prompt    = ChatPromptTemplate.from_messages([system_template, human_template])\n",
    "chain           = route_prompt | llm\n",
    "\n",
    "def get_routing_output(question: str, chat_history: List[Dict[str, str]] = [], retries: int = 2):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        start = time.time()\n",
    "        response = chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "        content = response.content.strip()\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(content)\n",
    "            result = RouteQuery(**parsed)\n",
    "            print(\n",
    "                f\"[✅ Attempt {attempt}] Took {time.time() - start:.2f}s → {result.datasource}\")\n",
    "            return result\n",
    "        except (json.JSONDecodeError, ValidationError):\n",
    "            print(f\"[⚠️ Attempt {attempt}] Failed to parse → {content}\")\n",
    "            if attempt == retries:\n",
    "                return None\n",
    "\n",
    "\n",
    "# Tests\n",
    "print(get_routing_output(\n",
    "    \"What are the type of business that require to implement e-invoice?\"))\n",
    "print(get_routing_output(\n",
    "    \"What is tax\"))\n",
    "print(get_routing_output(\n",
    "    \"Explain e-invoice\"))\n",
    "print(get_routing_output(\"Hey, how are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985950cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grade Document\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Prompt for grading\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keywords or has semantic meaning related to the user question, grade it as relevant, and return {{ \"binary_score\": \"yes\" }}. \n",
    "If the document not related to the user question,  return {{ \"binary_score\": \"no\" }}\n",
    "You are not required to be strict—just filter out obviously wrong matches.\n",
    "\n",
    "Respond ONLY with a single line JSON like this (no explanation, no extra text): {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\")\n",
    "])\n",
    "\n",
    "# Chain\n",
    "retrieval_grader = grade_prompt | llm\n",
    "\n",
    "# Helper function\n",
    "def grade_document(question: str, document: str):\n",
    "    response = retrieval_grader.invoke({\"question\": question, \"document\": document})\n",
    "    raw = response.content.strip()\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        result = GradeDocuments(**parsed)\n",
    "        return result\n",
    "    except Exception:\n",
    "        # Fallback handling for raw \"yes\" or \"no\"\n",
    "        if raw.lower() in [\"yes\", \"relevant\" '\"yes\"', \"'yes'\"]:\n",
    "            return GradeDocuments(binary_score=\"yes\")\n",
    "        elif raw.lower() in [\"no\", '\"no\"', \"'no'\"]:\n",
    "            return GradeDocuments(binary_score=\"no\")\n",
    "        else:\n",
    "            # IF NOT MATCH ANY OF THE ABOVE, GRADE AS YES, TO AVOID DATA MISSING\n",
    "            print(\"⚠️ Failed to parse grading result:\", raw)\n",
    "            return GradeDocuments(binary_score=\"yes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful and friendly assistant from LHDN (Lembaga Hasil Dalam Negeri Malaysia).\n",
    "\n",
    "Your goal is to think through the user's question logically and clearly. If it involves numbers or reasoning (e.g., tax calculations, income estimations), first break it down step by step in your mind. Then, use the information from the context below to support and verify your answer.\\n\n",
    "Do not mention the word \"context\", \"document\", or how you got the information. Just answer as if you are directly speaking to the user, in a clear and natural tone.\\n\n",
    "If the answer cannot be clearly found or supported, respond politely and conversationally, and let the user know you’re not certain.\\n\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\n",
    "Answer (with thoughtful reasoning and helpful explanation):\"\"\"\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4052117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model for grading hallucination\n",
    "\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "system = \"\"\"You are a grader that only will output 'yes' or 'no' that will be assessing whether an LLM generation is grounded in or supported by a set of retrieved facts.\n",
    "Return {{ \"binary_score\": \"yes\" }} if the answer is grounded in or supported by the set of facts, else return {{ \"binary_score\": \"no\" }}. \n",
    "Respond ONLY with a single line JSON like this (no explanation, no extra text): {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}.\n",
    "\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | llm\n",
    "\n",
    "def grade_hallucination(documents, generation):\n",
    "    # Prepare the input for the LLM\n",
    "    response = hallucination_grader.invoke({\n",
    "        \"documents\": documents,\n",
    "        \"generation\": generation\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response.content.strip())\n",
    "        result = GradeHallucinations(**parsed)\n",
    "        return result\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        print(\"⚠️ Failed to parse grading result:\", response)\n",
    "        return { \"binary_score\": \"yes\" }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\", \"no\"] =  Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "    'yes' means that the answer resolves the question. 'no' means that the answer doesn't resolve the question\n",
    "     Give a binary score {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}. \n",
    "     Respond ONLY with a single line JSON like this (no explanation, no extra text): {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}\n",
    "     \"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | llm\n",
    "\n",
    "def grade_answer(question, generation):\n",
    "    # Prepare the input for the LLM\n",
    "    response = answer_grader.invoke({\n",
    "        \"question\": question,\n",
    "        \"generation\": generation\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response.content.strip())\n",
    "        result = GradeAnswer(**parsed)\n",
    "        return result\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        print(\"⚠️ Failed to parse grading result:\", response)\n",
    "        return { \"binary_score\": \"yes\" }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Cleaner prompt with clear instructions\n",
    "re_write_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \"\"\"You are an assistant that rephrases vague or follow-up questions into clear, standalone questions.\n",
    "\n",
    "Use only the chat history to preserve the original meaning — do NOT add new information or change the topic.\n",
    "\n",
    "Your goal is to make the user's latest question understandable **without requiring the previous messages**.\n",
    "\n",
    "If the user question is unhappy with the previous output, use the latest chat history as ground, rewrite the user question and make it clearer\n",
    "\n",
    "\"\"\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"Chat history:\\n{chat_history}\\n\\nUser's question:\\n{question}\\n\\nRephrase the user's question so that it is standalone and retains the same meaning.\"\"\"\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Connect the prompt to your model and parser\n",
    "question_rewriter_with_history = (\n",
    "    re_write_with_history_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document genration\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | llm | StrOutputParser() \n",
    ")\n",
    "\n",
    "hyde_retrieval_chain = generate_docs_for_retrieval | retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd43b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        chat_history: list of chat history\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    chat_history: List[Dict[str, str]] = []\n",
    "    from_llm_direct: bool\n",
    "    retry_count: int = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6dba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RUNNING: retrieve---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def retrieve_with_HyDE(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RUNNING: retrieve---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    documents = hyde_retrieval_chain.invoke(question)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = grade_document(question, d.page_content)\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate an answer using RAG and maintain chat history.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        GraphState: Updated state with generation and chat history\n",
    "    \"\"\"\n",
    "    print(\"---RAG GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "\n",
    "    # Add user message\n",
    "    chat_history.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    # Add assistant's response to history\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": generation})\n",
    "\n",
    "    score = grade_hallucination(documents, generation)\n",
    "    print(score)\n",
    "\n",
    "    return GraphState(\n",
    "        question=question,\n",
    "        documents=documents,\n",
    "        generation=generation,\n",
    "        chat_history=chat_history,\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_llm_response(state: GraphState) -> GraphState:\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "\n",
    "    # Add current question\n",
    "    chat_history.append({\"role\": \"user\", \"content\": question})\n",
    "    print(chat_history)\n",
    "\n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(question)\n",
    "    answer = response.content.strip()\n",
    "\n",
    "    # Add response to history\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    return GraphState(\n",
    "        question=question,\n",
    "        generation=answer,\n",
    "        chat_history=chat_history\n",
    "    )\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, CHECK RETRY LIMIT---\"\n",
    "        )\n",
    "        return check_retry_limit(state)\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def check_retry_limit(state):\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    if retry_count < 1:\n",
    "        print(\n",
    "            \"---DECISION: RETRY LIMIT NOT EXCEED, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\n",
    "            \"---DECISION: LIMIT EXCEED, FALLBACK---\"\n",
    "        )\n",
    "        return \"llm_fallback\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = grade_hallucination(documents, generation)\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        usefulness = grade_answer(question,generation)\n",
    "        if(usefulness.binary_score == \"yes\"):\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            return \"not-useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"hallucinate\"\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to appropriate node based on the datasource and vector database.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- ROUTE QUESTION ---\")\n",
    "    question = state[\"question\"]\n",
    "    print(\"Question:\", question)\n",
    "\n",
    "    source = get_routing_output(question)\n",
    "    print(\"Datasource:\", source.datasource)\n",
    "\n",
    "    if source == None:\n",
    "        print(\"→ Source is None, fallback. Routing to: llm (direct)\")\n",
    "        return \"llm\"\n",
    "\n",
    "    if source.datasource == \"llm\":\n",
    "        print(\"→ Routing to: llm (direct)\")\n",
    "        state[\"from_llm_direct\"] = True\n",
    "        return \"llm\"\n",
    "\n",
    "    if source.datasource == \"vectorstore\":\n",
    "        return \"vectorstore\"\n",
    "\n",
    "    raise ValueError(f\"Unknown datasource: {source.datasource}\")\n",
    "\n",
    "def transform_query_with_history(state):\n",
    "    \"\"\"\n",
    "    Transform the user's query using chat history context to generate a more complete question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state containing 'question', optional 'documents', and 'chat_history'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with a refined 'question'.\n",
    "    \"\"\"\n",
    "    print(\"---TRANSFORM QUERY WITH HISTORY---\")\n",
    "\n",
    "    question = state.get(\"question\", \"\").strip()\n",
    "    chat_history = state.get(\"chat_history\", [])  # List of (user_msg, ai_msg) tuples\n",
    "\n",
    "    # Reconstruct chat context (last 5 turns max)\n",
    "    history_text = \"\\n\".join(\n",
    "        [f\"User: {user}\\nAI: {ai}\" for user, ai in chat_history[-3:]]\n",
    "    ).strip()\n",
    "\n",
    "    prompt_input = {\n",
    "        \"question\": question,\n",
    "        \"chat_history\": history_text\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        better_question = question_rewriter_with_history.invoke(prompt_input).strip()\n",
    "        print(f\"🔁 Rewritten Question: {better_question}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error rewriting question: {e}\")\n",
    "        better_question = question  # Fallback to original\n",
    "\n",
    "    return {\n",
    "        \"question\": better_question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"documents\": state.get(\"documents\", []),\n",
    "    }\n",
    "\n",
    "\n",
    "def smart_transform_query(state):\n",
    "    \"\"\"\n",
    "    Smartly transform query only if it's unclear or a follow-up.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Contains current question, chat history, and more\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): May update the 'question' key\n",
    "    \"\"\"\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    history = state.get(\"chat_history\", [])\n",
    "    documents = state.get(\"documents\", [])\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "\n",
    "    # Step 1: Use LLM to classify if the question needs rephrasing\n",
    "    need_transform_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \n",
    "        \"\"\"You are an assistant that determines whether a user's current question needs to be rephrased based on the prior conversation history.\n",
    "\n",
    "    Respond ONLY with 'yes' or 'no'.\n",
    "\n",
    "    Respond 'yes' if:\n",
    "    - The question contains vague or ambiguous language.\n",
    "    - The question includes pronouns or references to earlier messages (e.g., \"that\", \"this\", \"he\", \"she\", \"it\", \"the previous one\", \"your answer\", \"what you said\").\n",
    "    - The question is clearly a follow-up that relies on previous context (e.g., it adds new information or builds on a past response).\n",
    "    - The question expresses confusion, dissatisfaction, or critique of the previous answer (e.g., \"I don’t understand\", \"why is that?\", \"can you explain more?\", \"that doesn’t make sense\", \"you didn’t answer my question\", \"your answer is wrong\").\n",
    "\n",
    "    Respond 'no' if:\n",
    "    - The question is clearly stated and can stand on its own without needing to refer to any prior messages.\n",
    "    - Even if the user has asked something before, the current question is complete and self-contained.\n",
    "\n",
    "    Always analyze both the current question and the chat history carefully before deciding.\n",
    "\n",
    "    Chat history:\n",
    "    {history}\n",
    "\n",
    "    Current question:\n",
    "    {question}\n",
    "    \"\"\"),\n",
    "        (\"human\", \n",
    "        \"Here is the current question.\\n{question}\\nAnswer with 'yes' or 'no' only.\")\n",
    "    ])\n",
    "\n",
    "\n",
    "    chain = need_transform_prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"history\": format_history(history)\n",
    "    })\n",
    "\n",
    "    if response.strip().lower() == \"yes\" and history:\n",
    "        print(\"[smart_transform_query] Transforming question due to follow-up or unclear input.\")\n",
    "        # Rewrite using historical context\n",
    "        better_question = transform_query_with_history(state)[\"question\"]\n",
    "        return {\n",
    "            \"documents\": documents,\n",
    "            \"question\": better_question,\n",
    "            \"retry_count\": retry_count,\n",
    "            \"chat_history\": history\n",
    "        }\n",
    "    else:\n",
    "        print(\"[smart_transform_query] Using original question. No transformation needed.\")\n",
    "        return {\n",
    "            \"documents\": documents,\n",
    "            \"question\": question,\n",
    "            \"retry_count\": retry_count,\n",
    "            \"chat_history\": history\n",
    "        }\n",
    "\n",
    "\n",
    "def format_history(history):\n",
    "    return \"\\n\".join([f\"User: {q}\\nBot: {a}\" for q, a in history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Nodes\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "#workflow.add_node(\"retrieve_with_HyDE\", retrieve_with_HyDE)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"llm_fallback\", generate_llm_response)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"preprocess_prompt\", smart_transform_query)\n",
    "workflow.add_node(\"transform_query_with_history\",transform_query_with_history)\n",
    "\n",
    "workflow.add_edge(START, \"preprocess_prompt\")\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"preprocess_prompt\",\n",
    "    route_question,\n",
    "    {\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "        \"llm\": \"llm_fallback\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "#workflow.add_edge(\"retrieve_with_HyDE\", \"grade_documents\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query_with_history\",\n",
    "        \"llm_fallback\": \"llm_fallback\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_edge(\"transform_query_with_history\", \"retrieve\")\n",
    "#workflow.add_edge(\"transform_query_with_history\", \"retrieve_with_HyDE\")\n",
    "\n",
    "workflow.add_edge(\"llm_fallback\", END)\n",
    "\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"generate\",\n",
    "#     grade_generation_v_documents_and_question,\n",
    "#     {\n",
    "#         \"hallucinate\": \"generate\",\n",
    "#         \"useful\": \"END\"\n",
    "#         \"not-useful\": \"transform_query_with_history\",\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da01c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "print(\"🤖 Welcome! Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"👋 Goodbye!\")\n",
    "        break\n",
    "\n",
    "    state = {\n",
    "        \"question\": user_input,\n",
    "        \"chat_history\": chat_history,\n",
    "    }\n",
    "\n",
    "    print(user_input)\n",
    "\n",
    "    # Run through the LangGraph app\n",
    "    for output in app.stream(state):\n",
    "        for key, value in output.items():\n",
    "            if key == \"llm_fallback\" or key == \"generate\":  # depends on where generation happens\n",
    "                answer = value[\"generation\"]\n",
    "                chat_history = value[\"chat_history\"]\n",
    "                pprint(f\"\\nAI: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552271f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c5716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
