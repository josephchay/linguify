{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1933f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import Document\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Literal, Dict, List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryByteStore,InMemoryStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR_LANGSMITH_API_KEY\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"YOUR_LANGSMITH_PROJECT\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR_HUGGINGFACEHUB_API_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0b585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tt/wthpjbnn30v15mvkjnkfrd1c0000gn/T/ipykernel_26311/1373421289.py:5: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
      "/var/folders/tt/wthpjbnn30v15mvkjnkfrd1c0000gn/T/ipykernel_26311/1373421289.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/Users/leongtinjet/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Initialize embeddings\n",
    "#embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "qwen = ChatOllama(model=\"qwen2:1.5b\", temperature=0)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676ddfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tt/wthpjbnn30v15mvkjnkfrd1c0000gn/T/ipykernel_26311/360378128.py:6: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "PERSIST_DIRECTORY = os.path.join(BASE_DIR, \"vector-db\", \"lhdn_db_new\")\n",
    "COLLECTION_NAME = \"lhdn\"\n",
    "\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=PERSIST_DIRECTORY,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b93f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✅ Attempt 1] Took 14.04s → vectorstore\n",
      "datasource='vectorstore'\n",
      "[✅ Attempt 1] Took 0.41s → vectorstore\n",
      "datasource='vectorstore'\n",
      "[✅ Attempt 1] Took 0.42s → vectorstore\n",
      "datasource='vectorstore'\n",
      "[✅ Attempt 1] Took 0.37s → llm\n",
      "datasource='llm'\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from pydantic import BaseModel, ValidationError, Field\n",
    "import json\n",
    "import time\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"vectorstore\", \"llm\"]\n",
    "\n",
    "\n",
    "# LLM (Ollama)\n",
    "\n",
    "\n",
    "# Improved Prompt (clear, compact, and JSON-structured output)\n",
    "system = \"\"\"\n",
    "You are an intelligent assistant for LHDN (Lembaga Hasil Dalam Negeri Malaysia).\n",
    "\n",
    "Your task is to decide whether a user's question should be answered using a factual vector database (which includes complete information about Malaysian tax for individuals, companies, and e-invoicing), or by the LLM for general conversation.\n",
    "\n",
    "Use the chat history below if helpful:\n",
    "{chat_history}\n",
    "\n",
    "Rules:\n",
    "- If the question is about LHDN, taxes (personal, company), or e-invoicing → respond:\n",
    "  {{ \"datasource\": \"vectorstore\" }}\n",
    "- If the question is small talk, greeting, opinion-based, or unrelated to LHDN/tax/e-invoice → respond:\n",
    "  {{ \"datasource\": \"llm\" }}\n",
    "\n",
    "Respond with ONLY a single line of JSON (no explanation), like:\n",
    "{{ \"datasource\": \"vectorstore\" }}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(system)\n",
    "human_template  = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "route_prompt    = ChatPromptTemplate.from_messages([system_template, human_template])\n",
    "chain           = route_prompt | llm\n",
    "\n",
    "def get_routing_output(question: str, chat_history: List[Dict[str, str]] = [], retries: int = 2):\n",
    "    for attempt in range(1, retries + 1):\n",
    "        start = time.time()\n",
    "        response = chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "        content = response.content.strip()\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(content)\n",
    "            result = RouteQuery(**parsed)\n",
    "            print(\n",
    "                f\"[✅ Attempt {attempt}] Took {time.time() - start:.2f}s → {result.datasource}\")\n",
    "            return result\n",
    "        except (json.JSONDecodeError, ValidationError):\n",
    "            print(f\"[⚠️ Attempt {attempt}] Failed to parse → {content}\")\n",
    "            if attempt == retries:\n",
    "                return None\n",
    "\n",
    "\n",
    "# Tests\n",
    "print(get_routing_output(\n",
    "    \"What are the type of business that require to implement e-invoice?\"))\n",
    "print(get_routing_output(\n",
    "    \"What is tax\"))\n",
    "print(get_routing_output(\n",
    "    \"Explain e-invoice\"))\n",
    "print(get_routing_output(\"Hey, how are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "985950cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grade Document\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Prompt for grading\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keywords or has semantic meaning related to the user question, grade it as relevant, and return {{ \"binary_score\": \"yes\" }}. \n",
    "If the document not related to the user question,  return {{ \"binary_score\": \"no\" }}\n",
    "You are not required to be strict—just filter out obviously wrong matches.\n",
    "\n",
    "Respond ONLY with a single line JSON like this (no explanation, no extra text): {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\")\n",
    "])\n",
    "\n",
    "# Chain\n",
    "retrieval_grader = grade_prompt | llm\n",
    "\n",
    "# Helper function\n",
    "def grade_document(question: str, document: str):\n",
    "    response = retrieval_grader.invoke({\"question\": question, \"document\": document})\n",
    "    raw = response.content.strip()\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        result = GradeDocuments(**parsed)\n",
    "        return result\n",
    "    except Exception:\n",
    "        # Fallback handling for raw \"yes\" or \"no\"\n",
    "        if raw.lower() in [\"yes\", \"relevant\" '\"yes\"', \"'yes'\"]:\n",
    "            return GradeDocuments(binary_score=\"yes\")\n",
    "        elif raw.lower() in [\"no\", '\"no\"', \"'no'\"]:\n",
    "            return GradeDocuments(binary_score=\"no\")\n",
    "        else:\n",
    "            print(\"⚠️ Failed to parse grading result:\", raw)\n",
    "            return GradeDocuments(binary_score=\"yes\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeb1b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful and friendly assistant from LHDN (Lembaga Hasil Dalam Negeri Malaysia).\n",
    "\n",
    "Your goal is to think through the user's question logically and clearly. If it involves numbers or reasoning (e.g., tax calculations, income estimations), first break it down step by step in your mind. Then, use the information from the context below to support and verify your answer.\n",
    "\n",
    "Do not mention the word \"context\", \"document\", or how you got the information. Just answer as if you are directly speaking to the user, in a clear and natural tone.\n",
    "\n",
    "If the answer cannot be clearly found or supported, respond politely and conversationally, and let the user know you’re not certain.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\n",
    "Answer (with thoughtful reasoning and helpful explanation):\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# LLM\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4052117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model for grading hallucination\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "system = \"\"\"You are a grader that only will output 'yes' or 'no' that will be assessing whether an LLM generation is grounded in or supported by a set of retrieved facts.\n",
    "Return {{ \"binary_score\": \"yes\" }} if the answer is grounded in or supported by the set of facts, else return {{ \"binary_score\": \"no\" }}. \n",
    "Respond ONLY with a single line JSON like this (no explanation, no extra text): {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}.\n",
    "\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | llm\n",
    "\n",
    "def grade_hallucination(documents, generation):\n",
    "    # Prepare the input for the LLM\n",
    "    response = hallucination_grader.invoke({\n",
    "        \"documents\": documents,\n",
    "        \"generation\": generation\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response.content.strip())\n",
    "        result = GradeHallucinations(**parsed)\n",
    "        return result\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        print(\"⚠️ Failed to parse grading result:\", response)\n",
    "        return None\n",
    "\n",
    "# Test the grader with real data\n",
    "# graded_result = grade_hallucination(docs,generation)\n",
    "\n",
    "# print(graded_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef62f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\", \"no\"] =  Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "    'Yes' means that the answer resolves the question. 'No' means that the answer doesn't resolve the question\n",
    "     Give a binary score {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}. \n",
    "     Respond ONLY with a single line JSON like this (no explanation, no extra text): {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}\n",
    "     \"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | llm\n",
    "\n",
    "def grade_answer(question, generation):\n",
    "    # Prepare the input for the LLM\n",
    "    response = answer_grader.invoke({\n",
    "        \"question\": question,\n",
    "        \"generation\": generation\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response.content.strip())\n",
    "        result = GradeAnswer(**parsed)\n",
    "        return result\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        print(\"⚠️ Failed to parse grading result:\", response)\n",
    "        return None\n",
    "\n",
    "# Test the grader with real data\n",
    "# graded_result = grade_answer(question, generation)\n",
    "\n",
    "# print(graded_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5a5f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "     Finally produce a new question and return the ONLY the new question.\n",
    "     Don't include your reasoning process in your output.\n",
    "     \"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "# question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65bd2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "system =  \"\"\"You are an assistant that helps rephrase unclear or follow-up questions into clear, standalone questions.\n",
    "\n",
    "Use the chat history to understand the full context. Your goal is to rewrite the user's current question so that it makes sense on its own without needing prior messages.\n",
    "\n",
    "Be clear, natural, and concise. Keep the meaning the same — do not add new assumptions.\n",
    "\n",
    "Chat History:\n",
    "{history}\n",
    "\n",
    "User’s Follow-up Question:\n",
    "{question}\n",
    "\n",
    "Rephrased Standalone Question:\"\"\"\n",
    "\n",
    "re_write_with_history_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\",  \"\"\"\n",
    "Original Question:\n",
    "{question}\n",
    "\n",
    "Recent Chat History:\n",
    "{chat_history}\n",
    "\"\"\"),\n",
    "])\n",
    "\n",
    "question_rewriter_with_history = (\n",
    "    re_write_with_history_prompt\n",
    "    | qwen\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfd43b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "        chat_history: list of chat history\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    chat_history: List[Dict[str, str]] = []\n",
    "    from_llm_direct: bool\n",
    "    retry_count: int = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a6dba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RUNNING: Sentiment Analysis---\")\n",
    "\n",
    "    return \"peace\"\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RUNNING: retrieve---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = grade_document(question, d.page_content)\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate an answer using RAG and maintain chat history.\n",
    "\n",
    "    Args:\n",
    "        state (GraphState): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        GraphState: Updated state with generation and chat history\n",
    "    \"\"\"\n",
    "    print(\"---RAG GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "\n",
    "    # Add user message\n",
    "    chat_history.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    # Add assistant's response to history\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": generation})\n",
    "\n",
    "    return GraphState(\n",
    "        question=question,\n",
    "        documents=documents,\n",
    "        generation=generation,\n",
    "        chat_history=chat_history,\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_llm_response(state: GraphState) -> GraphState:\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])\n",
    "\n",
    "    # Add current question\n",
    "    chat_history.append({\"role\": \"user\", \"content\": question})\n",
    "    print(chat_history)\n",
    "\n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(question)\n",
    "    answer = response.content.strip()\n",
    "\n",
    "    # Add response to history\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    return GraphState(\n",
    "        question=question,\n",
    "        generation=answer,\n",
    "        chat_history=chat_history\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print(better_question)\n",
    "    return {\"documents\": documents, \"question\": better_question, \"retry_count\": retry_count + 1}\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, CHECK RETRY LIMIT---\"\n",
    "        )\n",
    "        return check_retry_limit(state)\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def check_retry_limit(state):\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    if retry_count < 1:\n",
    "        print(\n",
    "            \"---DECISION: RETRY LIMIT NOT EXCEED, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\n",
    "            \"---DECISION: LIMIT EXCEED, FALLBACK---\"\n",
    "        )\n",
    "        return \"llm_fallback\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    #score = #grade_hallucination(documents, generation)\n",
    "    grade = \"yes\" #score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "\n",
    "        score = grade_answer(question, generation)\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"hallucinate\"\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to appropriate node based on the datasource and vector database.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"--- ROUTE QUESTION ---\")\n",
    "    question = state[\"question\"]\n",
    "    print(\"Question:\", question)\n",
    "\n",
    "    source = get_routing_output(question)\n",
    "    print(\"Datasource:\", source.datasource)\n",
    "\n",
    "    if source == None:\n",
    "        print(\"→ Source is None, fallback. Routing to: llm (direct)\")\n",
    "        return \"llm\"\n",
    "\n",
    "    if source.datasource == \"llm\":\n",
    "        print(\"→ Routing to: llm (direct)\")\n",
    "        state[\"from_llm_direct\"] = True\n",
    "        return \"llm\"\n",
    "\n",
    "    if source.datasource == \"vectorstore\":\n",
    "        return \"vectorstore\"\n",
    "\n",
    "    raise ValueError(f\"Unknown datasource: {source.datasource}\")\n",
    "\n",
    "def transform_query_with_history(state):\n",
    "    \"\"\"\n",
    "    Transform the user's query using chat history context to generate a more complete question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state containing 'question', optional 'documents', and 'chat_history'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with a refined 'question'.\n",
    "    \"\"\"\n",
    "    print(\"---TRANSFORM QUERY WITH HISTORY---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    chat_history = state.get(\"chat_history\", [])  # This should be a list of tuples: [(user_msg, ai_msg), ...]\n",
    "\n",
    "    # Reconstruct context from chat history\n",
    "    history_text = \"\"\n",
    "    for i, (user_msg, ai_msg) in enumerate(chat_history[-5:]):  # Only use recent history to avoid context overload\n",
    "        history_text += f\"User: {user_msg}\\nAI: {ai_msg}\\n\"\n",
    "    \n",
    "    prompt_input = {\n",
    "        \"question\": question,\n",
    "        \"chat_history\": history_text.strip()\n",
    "    }\n",
    "\n",
    "    better_question = question_rewriter_with_history.invoke(prompt_input)\n",
    "    print(better_question)\n",
    "\n",
    "    return {\n",
    "        \"question\": better_question,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"documents\": state.get(\"documents\", []),\n",
    "    }\n",
    "\n",
    "def smart_transform_query(state):\n",
    "    \"\"\"\n",
    "    Smartly transform query only if it's unclear or a follow-up.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): Contains current question, chat history, and more\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): May update the 'question' key\n",
    "    \"\"\"\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    history = state.get(\"chat_history\", [])\n",
    "    documents = state.get(\"documents\", [])\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "\n",
    "    # Step 1: Use LLM to classify if the question needs rephrasing\n",
    "    need_transform_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \n",
    "        \"\"\"You are an assistant that determines whether a user's current question needs to be rephrased based on the prior conversation history.\n",
    "\n",
    "    Respond ONLY with 'yes' or 'no'.\n",
    "\n",
    "    Respond 'yes' if:\n",
    "    - The question contains vague or ambiguous language.\n",
    "    - The question includes pronouns or references to earlier messages (e.g., \"that\", \"this\", \"he\", \"she\", \"it\", \"the previous one\", \"your answer\", \"what you said\").\n",
    "    - The question is clearly a follow-up that relies on previous context (e.g., it adds new information or builds on a past response).\n",
    "    - The question is expressing confusion, dissatisfaction, or asking for clarification about a prior response (e.g., \"I don’t understand\", \"why is that?\", \"can you explain more?\", \"that doesn’t make sense\").\n",
    "\n",
    "    Respond 'no' if:\n",
    "    - The question is clearly stated and can stand on its own without needing to refer to any prior messages.\n",
    "    - Even if the user has asked something before, the current question is complete and self-contained.\n",
    "\n",
    "    Always analyze both the current question and the chat history carefully before deciding.\n",
    "        \"\"\"),\n",
    "        \n",
    "        (\"user\", \n",
    "        \"Chat history: {{history}}\\n\\nCurrent question: {{question}} \\n\\nAnswer with 'yes' or 'no' only.\")\n",
    "    ])\n",
    "\n",
    "\n",
    "    chain = need_transform_prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"history\": format_history(history)\n",
    "    })\n",
    "\n",
    "    if response.strip().lower() == \"yes\" and history:\n",
    "        print(\"[smart_transform_query] Transforming question due to follow-up or unclear input.\")\n",
    "        # Rewrite using historical context\n",
    "        better_question = transform_query_with_history(state)[\"question\"]\n",
    "        return {\n",
    "            \"documents\": documents,\n",
    "            \"question\": better_question,\n",
    "            \"retry_count\": retry_count,\n",
    "            \"chat_history\": history\n",
    "        }\n",
    "    else:\n",
    "        print(\"[smart_transform_query] Using original question. No transformation needed.\")\n",
    "        return {\n",
    "            \"documents\": documents,\n",
    "            \"question\": question,\n",
    "            \"retry_count\": retry_count,\n",
    "            \"chat_history\": history\n",
    "        }\n",
    "\n",
    "\n",
    "def format_history(history):\n",
    "    return \"\\n\".join([f\"User: {q}\\nBot: {a}\" for q, a in history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcfa6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Nodes\n",
    "workflow.add_node(\"sentiment_analysis\", sentiment_analysis)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "#workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "workflow.add_node(\"llm_fallback\", generate_llm_response)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"preprocess_prompt\", smart_transform_query)\n",
    "workflow.add_node(\"transform_query_with_history\",transform_query_with_history)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    sentiment_analysis,\n",
    "    {\n",
    "        \"peace\": \"preprocess_prompt\",\n",
    "        \"angry\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "### ROUTE QUESTION BRANCH ###\n",
    "workflow.add_conditional_edges(\n",
    "    \"preprocess_prompt\",\n",
    "    route_question,\n",
    "    {\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "        \"llm\": \"llm_fallback\",\n",
    "    },\n",
    ")\n",
    "\n",
    "### RETRIEVE BRANCH ###\n",
    "#workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"llm_fallback\": \"llm_fallback\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "### RETRY BRANCH ###\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_edge(\"llm_fallback\", END)\n",
    "\n",
    "### GENERATE BRANCH ###\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"hallucinate\": END,\n",
    "        \"useful\": END,\n",
    "        #TODO\n",
    "        \"not useful\": \"transform_query_with_history\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"transform_query_with_history\", \"retrieve\")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2da01c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Welcome! Type 'exit' to quit.\n",
      "\n",
      "my compnay have an annual revenue of Rm20 million, when should we implement e-invoice?\n",
      "---RUNNING: Sentiment Analysis---\n",
      "[smart_transform_query] Using original question. No transformation needed.\n",
      "--- ROUTE QUESTION ---\n",
      "Question: my compnay have an annual revenue of Rm20 million, when should we implement e-invoice?\n",
      "[✅ Attempt 1] Took 1.03s → vectorstore\n",
      "Datasource: vectorstore\n",
      "---RUNNING: retrieve---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "---RAG GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "('\\n'\n",
      " 'AI: Based on the information provided by the Inland Revenue Board of '\n",
      " 'Malaysia (IRBM), the implementation date for e-Invoice is determined based '\n",
      " 'on the annual turnover or revenue.\\n'\n",
      " '\\n'\n",
      " 'Since your company has an annual revenue of RM20 million, which falls within '\n",
      " 'the threshold mentioned in Table 1.1, you are required to issue and submit '\n",
      " \"e-Invoices for IRBM's validation according to the implementation timeline.\\n\"\n",
      " '\\n'\n",
      " 'According to the guidelines, the compliance obligation is from the issuance '\n",
      " 'of e-Invoice perspective. This means that even if your annual revenue '\n",
      " 'changes in subsequent years, the initial implementation timeline remains the '\n",
      " 'same.\\n'\n",
      " '\\n'\n",
      " 'In this case, since you have an annual revenue of RM20 million, which falls '\n",
      " \"within the threshold, your company's e-Invoice implementation date would be \"\n",
      " 'determined based on the 2022 tax return for year of assessment. However, I '\n",
      " \"couldn't find any specific information on when exactly the implementation \"\n",
      " 'date is set for a revenue of RM20 million.\\n'\n",
      " '\\n'\n",
      " 'I would recommend checking with the IRBM or consulting their guidelines more '\n",
      " \"closely to determine the exact implementation date for your company's \"\n",
      " 'e-Invoice. They may be able to provide more specific guidance based on your '\n",
      " \"company's individual circumstances.\\n\"\n",
      " '\\n'\n",
      " \"It's also worth noting that even if you don't meet the threshold, you can \"\n",
      " 'still start preparing for e-Invoice implementation and familiarizing '\n",
      " 'yourself with the requirements. This will help ensure a smooth transition '\n",
      " 'when the time comes.\\n')\n",
      "When will I receive my current year's tax refund?\n",
      "---RUNNING: Sentiment Analysis---\n",
      "[smart_transform_query] Using original question. No transformation needed.\n",
      "--- ROUTE QUESTION ---\n",
      "Question: When will I receive my current year's tax refund?\n",
      "[✅ Attempt 1] Took 1.01s → vectorstore\n",
      "Datasource: vectorstore\n",
      "---RUNNING: retrieve---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "---RAG GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "('\\n'\n",
      " \"AI: If you've submitted your Income Tax Return Form (ITRF) on or before the \"\n",
      " 'deadline via e-Filing, your tax refund should be processed within 30 working '\n",
      " 'days from the date of submission. This is according to the information '\n",
      " 'provided by LHDN.\\n'\n",
      " '\\n'\n",
      " \"On the other hand, if you've submitted your ITRF manually or through hand \"\n",
      " 'delivery after the deadline, it will take 90 working days for your refund to '\n",
      " 'be processed.\\n'\n",
      " '\\n'\n",
      " 'So, the processing time for your tax refund depends on how you submit your '\n",
      " \"ITRF. If it's via e-Filing before the deadline, you can expect your refund \"\n",
      " 'within 30 working days.\\n')\n",
      "what is e-invoice?\n",
      "---RUNNING: Sentiment Analysis---\n",
      "[smart_transform_query] Using original question. No transformation needed.\n",
      "--- ROUTE QUESTION ---\n",
      "Question: what is e-invoice?\n",
      "[✅ Attempt 1] Took 0.97s → vectorstore\n",
      "Datasource: vectorstore\n",
      "---RUNNING: retrieve---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "---RAG GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "('\\n'\n",
      " \"AI: So, you're asking about what an e-Invoice is? Well, let me break it down \"\n",
      " 'for you.\\n'\n",
      " '\\n'\n",
      " 'An e-Invoice is a digital representation of a transaction between a supplier '\n",
      " \"and a buyer. It's essentially a digital version of the traditional invoice \"\n",
      " 'or receipt that businesses used to exchange with each other.\\n'\n",
      " '\\n'\n",
      " 'Think of it like this: when you buy something from a store, you usually get '\n",
      " 'a physical receipt as proof of purchase. An e-Invoice works in a similar '\n",
      " \"way, but instead of being printed on paper, it's sent electronically to the \"\n",
      " 'buyer.\\n'\n",
      " '\\n'\n",
      " \"The key thing about an e-Invoice is that it's a standardized digital format \"\n",
      " 'that can be easily read and verified by both parties involved in the '\n",
      " 'transaction. This makes it easier for businesses to comply with tax laws and '\n",
      " 'regulations, as well as reduces the risk of errors or disputes.\\n'\n",
      " '\\n'\n",
      " 'In other words, an e-Invoice is like a digital contract that outlines the '\n",
      " 'terms of the sale or provision of services, including the amount due, '\n",
      " \"payment terms, and any relevant taxes or fees. It's a more efficient and \"\n",
      " \"secure way to conduct business transactions, and it's becoming increasingly \"\n",
      " 'popular as more businesses move online.\\n'\n",
      " '\\n'\n",
      " 'Does that make sense?\\n')\n",
      "What docoment will i need to submit when filling individual tax\n",
      "---RUNNING: Sentiment Analysis---\n",
      "[smart_transform_query] Using original question. No transformation needed.\n",
      "--- ROUTE QUESTION ---\n",
      "Question: What docoment will i need to submit when filling individual tax\n",
      "[✅ Attempt 1] Took 1.02s → vectorstore\n",
      "Datasource: vectorstore\n",
      "---RUNNING: retrieve---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "---RAG GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\n",
      "('\\n'\n",
      " \"AI: When filling out your Individual Tax Return Form (ITRF), you'll need to \"\n",
      " 'provide some supporting documents along with it. According to the '\n",
      " 'information provided, these documents are not mandatory but should be kept '\n",
      " 'for seven years starting from the year you submitted your ITRF.\\n'\n",
      " '\\n'\n",
      " 'Some examples of supporting documentation that you might need to submit '\n",
      " 'include:\\n'\n",
      " '\\n'\n",
      " '* EA/EC Form\\n'\n",
      " '* Dividen vouchers\\n'\n",
      " '* Insurance premium receipt\\n'\n",
      " '* Book purchase receipt\\n'\n",
      " '* Medical receipts\\n'\n",
      " '* Donation receipt\\n'\n",
      " '* Receipt of payment of zakat\\n'\n",
      " \"* Child's birth certificate\\n\"\n",
      " '* Marriage certificate\\n'\n",
      " '\\n'\n",
      " 'These documents are meant to help verify your income and expenses, as well '\n",
      " \"as any deductions or rebates you're eligible for. However, it's worth noting \"\n",
      " 'that the specific documents required may vary depending on your individual '\n",
      " 'circumstances.\\n'\n",
      " '\\n'\n",
      " \"If you're unsure about what documents you need to submit, I recommend \"\n",
      " 'checking out the Frequently Asked Questions section on the LHDN website, '\n",
      " 'which might have more detailed information on this topic. Alternatively, you '\n",
      " 'can also refer to the guide notes or explanatory notes available on the IRBM '\n",
      " 'official portal for further clarification.\\n')\n",
      "---TRANSFORM QUERY WITH HISTORY---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['chat_history', 'history', 'question'] Received: ['question', 'chat_history']\\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(user_input)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Run through the LangGraph app\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllm_fallback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# depends on where generation happens\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2356\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2350\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2351\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2352\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2353\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2354\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2355\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2356\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2357\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2359\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2361\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2362\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2363\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2364\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 271\u001b[39m, in \u001b[36mtransform_query_with_history\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    264\u001b[39m     history_text += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mai_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    266\u001b[39m prompt_input = {\n\u001b[32m    267\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question,\n\u001b[32m    268\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m\"\u001b[39m: history_text.strip()\n\u001b[32m    269\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m better_question = \u001b[43mquestion_rewriter_with_history\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28mprint\u001b[39m(better_question)\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    275\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: better_question,\n\u001b[32m    276\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m\"\u001b[39m: chat_history,\n\u001b[32m    277\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: state.get(\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m, []),\n\u001b[32m    278\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/.venv/lib/python3.11/site-packages/langchain_core/prompts/base.py:215\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    214\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1933\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1929\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1930\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1931\u001b[39m         output = cast(\n\u001b[32m   1932\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1933\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1935\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1936\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1937\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1938\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1941\u001b[39m         )\n\u001b[32m   1942\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1943\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/.venv/lib/python3.11/site-packages/langchain_core/prompts/base.py:188\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     _inner_input = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**_inner_input)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/.venv/lib/python3.11/site-packages/langchain_core/prompts/base.py:182\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    176\u001b[39m     example_key = missing.pop()\n\u001b[32m    177\u001b[39m     msg += (\n\u001b[32m    178\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    183\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: \"Input to ChatPromptTemplate is missing variables {'history'}.  Expected: ['chat_history', 'history', 'question'] Received: ['question', 'chat_history']\\nNote: if you intended {history} to be part of the string and not a variable, please escape it with double curly braces like: '{{history}}'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT \"",
      "During task with name 'transform_query_with_history' and id '033c8049-1394-3931-a98c-4b55032f2d0e'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "print(\"🤖 Welcome! Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"👋 Goodbye!\")\n",
    "        break\n",
    "\n",
    "    state = {\n",
    "        \"question\": user_input,\n",
    "        \"chat_history\": chat_history,\n",
    "    }\n",
    "\n",
    "    print(user_input)\n",
    "\n",
    "    # Run through the LangGraph app\n",
    "    for output in app.stream(state):\n",
    "        for key, value in output.items():\n",
    "            if key == \"llm_fallback\" or key == \"generate\":  # depends on where generation happens\n",
    "                answer = value[\"generation\"]\n",
    "                chat_history = value[\"chat_history\"]\n",
    "                pprint(f\"\\nAI: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25188ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
