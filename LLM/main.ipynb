{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b230539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_community tiktoken langchain-cohere langchainhub chromadb langchain langgraph  tavily-python langchain-huggingface sentence-transformers bs4 pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b337c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"Linguify\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "os.environ[\"COHERE_API_KEY\"] =  \"mlfIo3Ty8ShDxh2xjSxzm0jc1YIv1wQxRDuRaNqu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0362460c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Stored all documents in vector DB.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m client\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m embeddings = \u001b[43mCohereEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membed-english-light-v3.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m     55\u001b[39m documents = extract_all_pdf_content(\u001b[33m\"\u001b[39m\u001b[33m/Users/leongtinjet/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/finance/E-invoice-pdf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m chunks = split_text(documents)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/.venv/lib/python3.11/site-packages/langchain_cohere/embeddings.py:87\u001b[39m, in \u001b[36mvalidate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/.venv/lib/python3.11/site-packages/cohere/client.py:153\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, api_key, base_url, environment, client_name, timeout, httpx_client, thread_pool_executor, log_warning_experimental_features)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/.venv/lib/python3.11/site-packages/cohere/base_client.py:139\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, base_url, environment, client_name, token, timeout, follow_redirects, httpx_client)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/.venv/lib/python3.11/site-packages/httpx/_client.py:688\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, auth, params, headers, cookies, verify, cert, trust_env, http1, http2, proxy, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, default_encoding)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/.venv/lib/python3.11/site-packages/httpx/_client.py:731\u001b[39m, in \u001b[36m_init_transport\u001b[39m\u001b[34m(self, verify, cert, trust_env, http1, http2, limits, transport)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:153\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, verify, cert, trust_env, http1, http2, limits, proxy, uds, local_address, retries, socket_options)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Year 3 Module/Designing Intelligence Agent/linguify/.venv/lib/python3.11/site-packages/httpx/_config.py:40\u001b[39m, in \u001b[36mcreate_ssl_context\u001b[39m\u001b[34m(verify, cert, trust_env)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:770\u001b[39m, in \u001b[36mcreate_default_context\u001b[39m\u001b[34m(purpose, cafile, capath, cadata)\u001b[39m\n\u001b[32m    767\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(purpose)\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cafile \u001b[38;5;129;01mor\u001b[39;00m capath \u001b[38;5;129;01mor\u001b[39;00m cadata:\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m context.verify_mode != CERT_NONE:\n\u001b[32m    772\u001b[39m     \u001b[38;5;66;03m# no explicit cafile, capath or cadata but the verify mode is\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;66;03m# CERT_OPTIONAL or CERT_REQUIRED. Let's try to load default system\u001b[39;00m\n\u001b[32m    774\u001b[39m     \u001b[38;5;66;03m# root CA certificates for the given purpose. This may fail silently.\u001b[39;00m\n\u001b[32m    775\u001b[39m     context.load_default_certs(purpose)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import chromadb\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a single PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "def extract_all_pdf_content(directory):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            pdf_text = extract_text_from_pdf(file_path)\n",
    "            documents.append(Document(\n",
    "                page_content=pdf_text,\n",
    "                metadata={\"source\": file_name}\n",
    "            ))\n",
    "    return documents\n",
    "\n",
    "def split_text(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=50\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def store_in_vector_db(chunks):\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    metadatas = [chunk.metadata for chunk in chunks]\n",
    "    embedded_chunks = embeddings.embed_documents(texts)\n",
    "\n",
    "    client = chromadb.PersistentClient(path=\"/Users/leongtinjet/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/vector-db/e-invoice_db\")\n",
    "    collection = client.get_or_create_collection(name=\"finance_pdfs\")\n",
    "\n",
    "    ids = [str(i) for i in range(len(texts))]\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=embedded_chunks,\n",
    "        documents=texts,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "\n",
    "    print(\"✅ Stored all documents in vector DB.\")\n",
    "    return client\n",
    "\n",
    "embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\") \n",
    "documents = extract_all_pdf_content(\"/Users/leongtinjet/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/finance/E-invoice-pdf\")\n",
    "chunks = split_text(documents)\n",
    "store_in_vector_db(chunks)\n",
    "persist_directory = \"/Users/leongtinjet/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM/vector-db/e-invoice_db\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"finance_pdfs\",\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c81546",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Set embeddings\n",
    "embd = CohereEmbeddings(model=\"embed-english-light-v3.0\") \n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embd,\n",
    "    persist_directory=\"/Users/leongtinjet/Documents/Year 3 Module/Designing Intelligence Agent/linguify/LLM\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d52c818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n",
      "datasource='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from pydantic import BaseModel, ValidationError, Field\n",
    "import json\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ..., description=\"Route to vectorstore or web_search\",\n",
    "    )\n",
    "\n",
    "# LLM (Ollama)\n",
    "llm = ChatOllama(model=\"mistral:7b\")\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"\n",
    "You are an intelligent router that decides whether a user's question should be answered using a local vector database (vectorstore) or by performing a live web search.\n",
    "\n",
    "The vectorstore contains official documents and content about:\n",
    "- E-invoice requirements\n",
    "- Malaysian LHDN guidelines\n",
    "- Tax compliance\n",
    "- Invoice formats\n",
    "- Business eligibility for e-invoicing\n",
    "- E-invoice implementation steps\n",
    "\n",
    "If the question is about those topics, route it to 'vectorstore'. Otherwise, route to 'web_search'.\n",
    "Respond ONLY in this JSON format: {{ \"datasource\": \"vectorstore\" }} or {{ \"datasource\": \"web_search\" }}\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = route_prompt | llm\n",
    "\n",
    "def get_routing_output(question: str):\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    try:\n",
    "        content = response.content.strip()\n",
    "        parsed = json.loads(content)\n",
    "        result = RouteQuery(**parsed)\n",
    "        return result\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        print(\"⚠️ Failed to parse response:\", response.content)\n",
    "        return None\n",
    "\n",
    "# Test it\n",
    "print(get_routing_output(\"Who will the Bears draft first in the NFL draft?\"))\n",
    "print(get_routing_output(\"What are the type of business that require to implement e-invoice?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "744dc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grade Document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import json\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Use Ollama model (you can change to 'llama2', 'gemma', etc. if you like)\n",
    "llm = ChatOllama(model=\"mistral:7b\", temperature=0)\n",
    "\n",
    "# Prompt for grading\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keywords or has semantic meaning related to the user question, grade it as relevant.\n",
    "You are not required to be strict—just filter out obviously wrong matches.\n",
    "\n",
    "Return ONLY a JSON object in this format: {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Retrieved document:\\n\\n{document}\\n\\nUser question: {question}\")\n",
    "])\n",
    "\n",
    "# Chain\n",
    "retrieval_grader = grade_prompt | llm\n",
    "\n",
    "# Helper function\n",
    "def grade_document(question: str, document: str):\n",
    "    response = retrieval_grader.invoke({\"question\": question, \"document\": document})\n",
    "    try:\n",
    "        parsed = json.loads(response.content.strip())\n",
    "        result = GradeDocuments(**parsed)\n",
    "        return result\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        print(\"⚠️ Failed to parse grading result:\", response.content)\n",
    "        return None\n",
    "\n",
    "question = \"What are the type of business that require to implement e-invoice?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6974384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The type of businesses that require e-invoice implementation include Associations, Bodies of persons, Branches, Business trusts, Co-operative societies, Corporations, Limited liability partnerships, Partnerships, Property trust funds, Properties trusts, Real estate investment trusts, Representative offices and regional offices, Trust bodies, and Unit trusts. Additionally, individuals conducting a business are also required to issue e-invoices.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"mistral:7b\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c55978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model for grading hallucination\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "llm = ChatOllama(model=\"mistral:7b\", temperature=0)\n",
    "\n",
    "system = \"\"\"You are a grader that only will output 'yes' or 'no' that will be assessing whether an LLM generation is grounded in or supported by a set of retrieved facts.\n",
    "Give a binary score 'yes' if the answer is grounded in or supported by the set of facts, else give 'no'. \n",
    "Return ONLY a JSON object in this format: {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}.\n",
    "\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | llm\n",
    "\n",
    "def grade_hallucination(documents, generation):\n",
    "    # Prepare the input for the LLM\n",
    "    response = hallucination_grader.invoke({\n",
    "        \"documents\": documents,\n",
    "        \"generation\": generation\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response.content.strip())\n",
    "        result = GradeHallucinations(**parsed)\n",
    "        return result\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        print(\"⚠️ Failed to parse grading result:\", response)\n",
    "        return None\n",
    "\n",
    "# Test the grader with real data\n",
    "graded_result = grade_hallucination(docs,generation)\n",
    "\n",
    "print(graded_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab92394f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOllama(model=\"mistral:7b\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\n",
    "     Return ONLY a JSON object in this format: {{ \"binary_score\": \"yes\" }} or {{ \"binary_score\": \"no\" }}\n",
    "     \"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | llm\n",
    "\n",
    "def grade_answer(question, generation):\n",
    "    # Prepare the input for the LLM\n",
    "    response = answer_grader.invoke({\n",
    "        \"question\": question,\n",
    "        \"generation\": generation\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response.content.strip())\n",
    "        result = GradeHallucinations(**parsed)\n",
    "        return result\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        print(\"⚠️ Failed to parse grading result:\", response)\n",
    "        return None\n",
    "\n",
    "# Test the grader with real data\n",
    "graded_result = grade_answer(question, generation)\n",
    "\n",
    "print(graded_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b779b8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"List businesses that necessitate e-invoicing implementation\" or \"What types of businesses should adopt e-invoicing?\"'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"mistral:7b\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ab69cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff18b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fadda892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = grade_document(question,d.page_content) \n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}\n",
    "\n",
    "\n",
    "### Edges ###\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = get_routing_output(question)\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = grade_hallucination(documents,generation)\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "       \n",
    "        score = grade_answer(question,generation)\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e5c1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\n",
    "    \"question\": \"What player at the Bears expected to draft first in the 2024 NFL draft?\"\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a16fce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "(' E-invoices are required in businesses such as associations, bodies of '\n",
      " 'persons, branches, business trusts, cooperative societies, corporations, '\n",
      " 'limited liability partnerships, partnerships, property trust funds, '\n",
      " 'properties trusts, real estate investment trusts, representative offices and '\n",
      " 'regional offices, trust bodies, and unit trusts. Additionally, individual '\n",
      " 'landlords conducting a business are also required to issue e-invoices to '\n",
      " 'tenants.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Run\n",
    "inputs = {\"question\": \"What are the type of business that require to implement e-invoice?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
